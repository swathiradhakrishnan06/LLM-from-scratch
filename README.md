# Large Language Models (LLM) From Scratch

> _This content is inspired by Dr. Raj Dandekarâ€™s YouTube playlist on Large Language Models. It will be updated as and when more topics are covered._

---

## ðŸ“‘ Table of Contents

1. [ðŸ“˜ Large Language Models (LLM) Basics](#-large-language-models-llm-basics)
2. [ðŸ§ª Pretraining LLMs vs Finetuning LLMs](#-pretraining-llms-vs-finetuning-llms)
3. [ðŸ¤– Understanding Transformers and LLMs](#-understanding-transformers-and-llms)
4. [ðŸ“š The Evolution and Core Mechanics of GPT Models](#-the-evolution-and-core-mechanics-of-gpt-models)

---

## ðŸ“˜ Large Language Models (LLM) Basics

### 1. What Exactly is a Large Language Model (LLM)?

At its simplest, **an LLM is a neural network designed to understand, generate, and respond to human-like text**. Key components include:

* **Neural Network**:
  Deep neural networks, inspired by the human brain, consist of layers of interconnected "neurons" that process input data. These networks are used for tasks such as:

  * Image detection
  * Text generation
  * Autonomous vehicles

* **Human-like Text Processing**:

  * **Understanding**: Comprehending human text.
  * **Generating**: Creating new text.
  * **Responding**: Interacting based on input prompts.

> *Example*: ChatGPT can understand prompts like â€œplan a relaxing dayâ€ and respond conversationally with a detailed itinerary.

> *In short*: LLMs are **deep neural networks trained on massive datasets** to perform tasks like understanding, generating, and responding to human-like text â€” often in a human-like way.

---

### 2. The Meaning of â€œLargeâ€ in LLM

The "large" refers to the **number of parameters** in the model.

* **Scale**:

  * Models like GPT-3 have up to **175 billion parameters**.
  * GPT-4 has **even more**.
* **Growth**:

  * There has been an **exponential increase** in model size, especially post-2020.
  * Language models dominate this growth.

> The sheer **scale of parameters** differentiates LLMs from earlier, smaller models.

---

### 3. The Meaning of â€œLanguage Modelâ€

A **language model** only deals with **textual data**, not images or videos.

LLMs are capable of performing various **Natural Language Processing (NLP)** tasks such as:

* Question Answering
* Translation
* Sentiment Analysis
* Text Completion

---

### 4. Difference Between Modern LLMs and Earlier NLP Models

#### ðŸ†š Earlier NLP Models:

* **Task-Specific**: One model = one task (e.g., only translation or only sentiment analysis)
* **Limited Capabilities**: Could not handle complex instructions like generating emails.

#### âœ… Modern LLMs:

* **General-Purpose**: One model = many NLP tasks.
* **Highly Capable**: Can write emails, generate articles, translate, etc.
* **Extremely Flexible**: Applications are broad and still expanding.

---

### 5. The "Secret Sauce": Transformer Architecture

The **Transformer** is the breakthrough architecture behind LLMs.

* **Origin**: Introduced in the 2017 paper **â€œAttention Is All You Needâ€** by Google Brain.
* **Impact**: Over **100,000 citations** â€” revolutionized AI.
* **Core Concepts**:

  * Input Embeddings
  * Multi-Head Attention
  * Positional Encoding

> While complex, understanding the Transformer is **essential to grasping how LLMs work**.

---

### 6. Understanding AI, ML, DL, LLM, and Generative AI Terminologies

#### ðŸ¤– Artificial Intelligence (AI)

> Any system that mimics human intelligence.
> Includes rule-based systems (e.g., scripted chatbots).

#### ðŸ“Š Machine Learning (ML)

> Subset of AI.
> Learns and adapts based on data and user interactions. Includes neural networks, decision trees, etc.

#### ðŸ§  Deep Learning (DL)

> Subset of ML using **neural networks only**.
> Examples: image classifiers, speech recognizers.

#### ðŸ“ Large Language Models (LLMs)

> Subset of DL for **text-based applications only**.
> Do **not** handle images or videos.

#### ðŸ§¬ Generative AI

> Combines DL + LLMs to **generate new content**.
> Covers multiple media types: text, images, videos, sound, etc.

#### ðŸ“Œ Summary Hierarchy:

```
AI > ML > DL > LLM
Generative AI = DL + LLM + multimodal content generation
```

---

### 7. Applications of LLMs

LLMs are widely used across many domains. Key application areas:

#### âœï¸ Content Creation

* Poems, books, news articles, social media content.

#### ðŸ¤– Chatbots & Virtual Assistants

* Used in banks, airlines, restaurants for automated customer support.

#### ðŸŒ Machine Translation

* Translates across languages, including regional ones.

#### ðŸ†• New Text Generation

* Generates original text content on any topic.

#### ðŸ˜Š Sentiment Analysis

* Detects tone, mood, or hate speech from text.

#### ðŸ› ï¸ Additional Tools Enabled by LLMs:

* YouTube Script Generator
* MCQ Generator
* Text Summarizer
* Text Rewriter
* Lesson Plan Generator

> These tools save time and effort in education, media, business, and more.

---

## ðŸ§ª Pretraining LLMs vs Finetuning LLMs

### 1. Overview of LLM Building Stages

Building an LLM primarily involves **two distinct but interconnected stages**:

* **Pre-training**: Teaches the model to understand and generate human-like language using a broad and diverse dataset.
* **Fine-tuning**: Adapts the model to specific tasks, domains, or industries using narrower, task-specific datasets.

---

### 2. Pre-training: The Foundational Stage

#### 2.1. Definition and Purpose

Pre-training is the **initial and most resource-intensive phase** where the model learns the structure of human language. Initially, LLMs were trained for **word completion**â€”predicting the next word in a sentence.

> "How is it able to interact so effectively with humans? How can it understand and respond so accurately?"

#### 2.2. Data Requirements

Pre-training requires **unlabeled, raw text data** from a variety of large-scale sources:

| Dataset      | Description                         | Size              |
| ------------ | ----------------------------------- | ----------------- |
| Common Crawl | Open internet data                  | 410 billion words |
| WebText2     | Reddit, blogs, Stack Overflow, code | 20 billion words  |
| Books        | Digitized book content              | 67 billion words  |
| Wikipedia    | Encyclopedia articles               | 3 billion words   |

> Example: GPT-3 was trained on **300 billion tokens** (â‰ˆ 300 billion words).

This is similar to how **children learn from parents and surroundings**, absorbing knowledge from all interactions.

#### 2.3. Computational Cost and Scale

* Extremely resource-heavy: **GPT-3â€™s pre-training cost was \~\$4.6 million**
* Requires **powerful GPUs or TPUs**
* Not feasible for individuals or small teams

> â€œNot possible for normal students or even for people without access to enterprise infrastructure.â€

#### 2.4. Capabilities of Pre-trained Models

Pre-trained models (a.k.a. **foundational models**) generalize well across many tasks **without being explicitly trained for each**:

* Text translation
* Question answering
* Sentiment analysis
* Summarization
* Linguistic acceptability
* Generating MCQs

> "One model can do all of these tasks on its own without ever being trained for these tasks."

---

### 3. Fine-tuning: Specialisation and Refinement

#### 3.1. Definition and Purpose

Fine-tuning involves **adapting a pre-trained model** to perform a **specific task or serve a specific industry**.

> "Itâ€™s a refinement on pre-training using a much narrower, labeled dataset."

#### 3.2. Data Requirements

Fine-tuning uses **labeled datasets**, such as:

* **Instruction fine-tuning**: e.g., pairs like (Prompt, Ideal Response)
* **Classification**: e.g., emails labeled as spam vs non-spam

#### 3.3. Use Cases and Examples

| Company            | Application              | Outcome / Notes                                          |
| ------------------ | ------------------------ | -------------------------------------------------------- |
| âœˆï¸ Airline Company | Customer Support Bot     | Responses specific to their own flight data & policies   |
| ðŸ“˜ Educational Co. | Question Generation Tool | High-quality, subject-specific questions                 |
| â˜Žï¸ SK Telecom      | Korean Telecom Bot       | +35% conversation summarization, +33% intent recognition |
| âš–ï¸ Harvey (Legal)  | Legal Assistant          | Trained on case law & legal documents                    |
| ðŸ¦ JP Morgan Chase | Enterprise AI Assistant  | Uses internal data; not possible with GPT alone          |

> "Foundational models are good for general users; industries **require** fine-tuning."

#### 3.4. When is Fine-tuning Necessary?

* When moving to **production-level deployment**
* For **startups or enterprises** with proprietary data
* For **specialised accuracy**, personalization, or regulatory needs

> "Big companies never use just the foundational modelâ€”they always take the next step of fine-tuning."

---

### 4. Schematic Overview of LLM Building

```
ðŸ§© DATA COLLECTION
    â¬‡
ðŸ› ï¸ PRE-TRAINING (Foundational Model)
    - Unlabeled data
    - Expensive, resource-intensive
    - General-purpose capabilities
    â¬‡
ðŸŽ¯ FINE-TUNING (Custom Model)
    - Labeled, narrow domain data
    - Task-specific or company-specific outputs
```

> Result: A **fine-tuned LLM** capable of real-world applications like summarizers, translators, legal bots, or customer service assistants.

---

## ðŸ¤– Understanding Transformers and LLMs

### 1. The Transformer: The â€œSecret Sauceâ€ of Modern LLMs

* Transformers are the **core architecture** powering most modern Large Language Models (LLMs).
* Introduced in the landmark 2017 paper *["Attention Is All You Need"](https://arxiv.org/abs/1706.03762)*, which has received **100,000+ citations**.
* Originally developed for **machine translation tasks** (e.g., English to German), not for text generation.
* GPT (Generative Pre-trained Transformer)â€”the foundation of ChatGPTâ€”is **built on the Transformer architecture**.

---

### 2. Simplified Transformer Architecture: 8-Step Flow

An intuitive walkthrough of how a Transformer translates text, e.g., English to German:

1. **Input Text**

   * Example: â€œthis is an exampleâ€

2. **Pre-processing (Tokenization)**

   * The input is split into **tokens** and mapped to **IDs**
   * Note: Tokens are not always equivalent to words.

3. **Encoder & Vector Embedding**

   * Each token is converted into a **high-dimensional vector**, capturing **semantic relationships**
   * Example: Vectors for *â€œdogâ€* and *â€œpuppyâ€* are close together

4. **Generated Vector Embeddings**

   * Output from encoder: contextualized vector representations of input

5. **Partial Output Text (Decoder Input)**

   * Decoder receives the partially generated output (e.g., â€œDas istâ€¦â€)

6. **Decoder Input**

   * Decoder takes both **encoder embeddings** and **partial output** to continue generating

7. **Word Prediction by Decoder**

   * Decoder predicts the **next word/token** based on context

8. **Final Output**

   * Full translated sentence is generated
   * Learning is improved via a **loss function**

---

### 3. Key Components: Encoder, Decoder & Self-Attention

| Component          | Purpose                                                          |
| ------------------ | ---------------------------------------------------------------- |
| **Encoder**        | Converts input tokens into contextual embeddings                 |
| **Decoder**        | Generates output based on embeddings and prior output            |
| **Self-Attention** | Core innovation: enables model to focus on relevant words/tokens |

#### ðŸ” Self-Attention Explained:

* **Assigns attention scores** to input tokens based on importance for predicting the next word
* Enables understanding of **long-range dependencies**

  > Like remembering what happened **pages ago** in a novel
* Helps model decide **which words to "attend to"** at each step

---

### 4. Transformer Variants: BERT vs GPT

#### ðŸ§  BERT (Bidirectional Encoder Representations from Transformers)

* **Task**: Predict masked words in a sentence (Masked Language Modeling)
* **Bidirectional**: Looks at context **before and after** each word
* **Architecture**: Uses **only the encoder**
* **Use Cases**: Sentiment analysis, classification tasks

#### âœï¸ GPT (Generative Pre-trained Transformer)

* **Task**: Predict the **next word** in a sequence (causal language modeling)
* **Unidirectional**: Left-to-right only
* **Architecture**: Uses **only the decoder**
* **Use Cases**: Text generation, summarization, chatbots (e.g., ChatGPT)

| Feature        | BERT                | GPT                            |
| -------------- | ------------------- | ------------------------------ |
| Directionality | Bidirectional       | Unidirectional (left to right) |
| Component Used | Encoder only        | Decoder only                   |
| Use Case       | Understanding tasks | Generative tasks               |

---

### 5. Transformers â‰  LLMs

> âš ï¸ The terms â€œTransformersâ€ and â€œLLMsâ€ are often **confused** but are **not the same**.

#### Not All Transformers Are LLMs:

* Transformers can be used in **non-language** domains like vision:

  * Example: **Vision Transformers (ViT)** outperform CNNs in tasks like:

    * Image classification
    * Tumor detection
    * Road pothole detection

#### Not All LLMs Are Transformers:

* LLMs can be built using **other architectures** like:

  * **RNNs (Recurrent Neural Networks)** â€“ introduced in 1980
  * **LSTMs (Long Short-Term Memory)** â€“ introduced in 1997
* These models handled **sequence prediction** tasks and had memory mechanisms:

  * **RNNs**: Feedback loop for short-term memory
  * **LSTMs**: Paths for both short-term and long-term memory

#### âœ… Summary:

* **Transformer** = An architecture (neural network design)
* **LLM** = A model that understands/generates language (can use different architectures)

> **Most modern LLMs use Transformer architecture, but not all Transformers are LLMs.**

---

## ðŸ“š The Evolution and Core Mechanics of GPT Models

### 1. ðŸ•°ï¸ Historical Progression of GPT Models

#### ðŸ”¹ Transformers (2017): â€œAttention Is All You Needâ€

* Introduced **self-attention mechanism** to model long-range dependencies.
* Replaced RNNs and LSTMs in many NLP tasks.
* Architecture had both **encoder and decoder** blocks.

#### ðŸ”¹ GPT-1 (2018): *â€œImproving Language Understanding with Unsupervised Learningâ€*

* Introduced the **Generative Pre-training** paradigm.
* Removed the encoder â€“ GPT models are **decoder-only**.
* Trained on large-scale **unlabeled text** to predict the **next word** (unsupervised learning).

#### ðŸ”¹ GPT-2 (2019): *â€œLanguage Models are Unsupervised Multitask Learnersâ€*

* Scaled up both **model size** and **dataset**.
* Largest version had **1.5 billion parameters**.
* Demonstrated **multi-task** capabilities without task-specific training.

#### ðŸ”¹ GPT-3 (2020): *â€œLanguage Models are Few-Shot Learnersâ€*

* Massive scale-up to **175 billion parameters**.
* Showed **few-shot**, **one-shot**, and **zero-shot** learning capabilities.
* Could translate, answer questions, recognize emotion, etc., with **few or no examples**.

#### ðŸ”¹ GPT-3.5 and GPT-4 (2022â€“Present)

* GPT-3.5: Sparked **commercial virality** (ChatGPT release).
* GPT-4: Represents the **state-of-the-art** with improved reasoning and accuracy.

---

### 2. ðŸ§  Zero-Shot, One-Shot, and Few-Shot Learning

| Learning Type | Description                                | Example                                                   |
| ------------- | ------------------------------------------ | --------------------------------------------------------- |
| **Zero-Shot** | Performs a task with **no prior examples** | Translate â€œcheeseâ€ â†’ â€œfromageâ€ with only task instruction |
| **One-Shot**  | One example provided                       | â€œSea otter â†’ loutre de merâ€ before translating â€œcheeseâ€   |
| **Few-Shot**  | Few examples provided                      | 3â€“5 translation examples before asking model to translate |

> ðŸ“ GPT-3 and GPT-4: Primarily **few-shot learners**, but also excel in **zero-shot** settings.

---

### 3. ðŸ“Š Data Scale and Training Costs

#### ðŸ“ Dataset Sources

* **300 billion tokens** used for GPT-3 training.
* Major components:

  * **Common Crawl**: 410B tokens (\~60%)
  * **WebText2**: 19B tokens (\~22%)
  * **Books + Wikipedia**: \~18%

#### ðŸ”¢ Tokenization

* A **token** is the modelâ€™s smallest meaningful unit â€“ not always equal to one word.

#### ðŸ’° Training Cost

* **\$4.6 million** estimated cost for GPT-3 pre-training.
* Driven by:

  * Size: **175B parameters**
  * Resource: **GPU/TPU compute**

#### ðŸ§ª Pre-training vs. Fine-tuning

| Stage            | Description                                                                         |
| ---------------- | ----------------------------------------------------------------------------------- |
| **Pre-training** | Unsupervised learning on **diverse, large datasets**                                |
| **Fine-tuning**  | Supervised learning on **narrow, domain-specific data** for real-world applications |

---

### 4. ðŸ—ï¸ GPT Architecture and Auto-Regressive Nature

#### ðŸ”§ Architecture

* GPT uses **only the decoder** from the Transformer.
* GPT-3 includes **96 Transformer layers**.

#### ðŸ” Auto-Regressive Property

* Predicts text **one token at a time** using previous tokens.
* Previous output becomes part of the **input** for the next prediction.

#### ðŸ§© Self-Supervised Learning

* Label = next word in sentence â†’ No need for manual annotations.
* Trained by minimizing difference between **predicted** and **true** next word.

---

### 5. ðŸŒŸ Emergent Behaviour

#### ðŸ“Œ Definition

> "The ability of a model to perform tasks it wasnâ€™t explicitly trained for."

#### ðŸ’¡ Surprising Capabilities

* Language translation
* Essay grading
* Multiple-choice question generation
* Text summarization
* Emotional recognition

#### ðŸ” Research Frontier

* Emergent abilities remain **not fully understood**.
* A major area of **active research** in AI.

---

### 6. ðŸ§¬ Open-Source vs. Closed-Source Models

| Model Type        | Description                                             |
| ----------------- | ------------------------------------------------------- |
| **Closed-Source** | Parameters are hidden; access via API (e.g., GPT-4)     |
| **Open-Source**   | Full model weights publicly available (e.g., LLaMA 3.1) |

#### ðŸ“ˆ Performance Shift

* LLaMA 3.1 has **405B parameters** and **approaches or beats GPT-4** in benchmarks.
* **Open-source is catching up** in performance and accessibility.

---

### ðŸ§  Summary

* GPT models evolved from **decoder-only** variants of Transformers, optimized for **next-word prediction**.
* Their scale enables **few-shot learning** and **emergent behaviour** across tasks.
* While GPTs are currently **closed-source** in OpenAIâ€™s offerings, **open-source alternatives** are rapidly closing the gap.
* GPTâ€™s power lies in its **auto-regressive, unsupervised training** and **self-attention mechanism**, which together allow surprisingly **versatile capabilities** without task-specific training.

---

### ðŸ“š Acknowledgement

> ðŸ§  *This content is inspired by Dr. Raj Dandekarâ€™s YouTube playlist on Large Language Models.*
> It has been adapted for educational purposes into a markdown-friendly format for easy reference and learning.

---
